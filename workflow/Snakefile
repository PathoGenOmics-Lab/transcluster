import re
import json
import requests
from pathlib import Path

configfile: "config/config.yaml"


def read_fasta_IDs(path):
    ids = []
    with open(path) as f:
        for line in f:
            if line.startswith(">"):
                ids.append(line[1:].strip().split(" ")[0])
    return ids


def download_file(url, path):
    response = requests.get(url)
    with open(path, "wb") as fw:
        fw.write(response.content)


def copy_file(from_path, to_path, chunk_size=1024):
    with open(from_path, "rb") as f:
        with open(to_path, "wb") as fw:
            fw.write(f.read())


INPUT_DIR = Path(config["INPUT_DIR"])
OUTPUT_DIR = Path(config["OUTPUT_DIR"])
DATASETS = [path.stem for path in INPUT_DIR.glob("*.txt")]


rule all:
    input:
        expand(OUTPUT_DIR/"{dataset}/clusters.csv", dataset=DATASETS),
        expand(OUTPUT_DIR/"{dataset}/summary.json", dataset=DATASETS),
        expand(OUTPUT_DIR/"{dataset}/fitness/{days}d/[{prop}]_[{size}]/estimated_fitness.csv", dataset=DATASETS, days=config["FITNESS_PADDING_DAYS"], prop=config["MIN_PROP"], size=config["MIN_SIZE"]),
        expand(OUTPUT_DIR/"summary/fitness/{days}d/[{prop}]_[{size}]/report.svg", days=config["FITNESS_PADDING_DAYS"], prop=config["MIN_PROP"], size=config["MIN_SIZE"]),
        expand(OUTPUT_DIR/"{dataset}/timeline/timeline.svg", dataset=DATASETS),
        OUTPUT_DIR/"summary/timeline/timeline.svg",
        OUTPUT_DIR/"summary/age/transmission_age.svg",
        expand(OUTPUT_DIR/"{dataset}/age/vs_background.svg", dataset=DATASETS),
        OUTPUT_DIR/"summary/missing/age.svg"


rule get_problematic_vcf:
    threads: 1
    shadow: "shallow"
    output:
        vcf_path = "data/problematic_sites_sarsCov2.vcf"
    resources:
        mem_mb = 2000,
        runtime = 15
    retries: 3
    run:
        if config["PROBLEMATIC_VCF"].startswith("https://"):
            download_file(config["PROBLEMATIC_VCF"], output.vcf_path)
        else:
            copy_file(config["PROBLEMATIC_VCF"], output.vcf_path)


rule get_reference_tree:
    threads: 1
    shadow: "shallow"
    output:
        tree_path = "data/public.all.masked.pb.gz"
    resources:
        mem_mb = 2000,
        runtime = 15
    retries: 3
    run:
        if config["REFERENCE_TREE"].startswith("https://"):
            download_file(config["REFERENCE_TREE"], output.tree_path)
        else:
            copy_file(config["REFERENCE_TREE"], output.tree_path)


rule extract_records:
    threads: 1
    shadow: "shallow"
    conda: "envs/bio.yaml"
    input:
        ids_file = INPUT_DIR/"{dataset}.txt"
    params:
        full_fasta = config["ALIGNED_FULL_FASTA"]
    output:
        fasta = OUTPUT_DIR/"{dataset}/sequences.aligned.fasta",
        extracted_ids = OUTPUT_DIR/"{dataset}/extracted_ids.csv",
        targets = OUTPUT_DIR/"{dataset}/targets.txt"
    resources:
        mem_mb = 4000,
        runtime = 60
    script:
        "scripts/extract_records.py"


rule phylogenetic_placement:
    threads: 8
    shadow: "shallow"
    conda: "envs/usher.yaml"
    input:
        new_samples = OUTPUT_DIR/"{dataset}/sequences.aligned.fasta",
        masking_vcf = "data/problematic_sites_sarsCov2.vcf",
        tree_protobuf = "data/public.all.masked.pb.gz"
    params:
        reference_fasta = config["REFERENCE_FASTA"],
        reference_id = read_fasta_IDs(config["REFERENCE_FASTA"])[0],
        output_directory = "pp_out"  # if shadow == "shallow", it will be removed
    output:
        tree = OUTPUT_DIR/"{dataset}/tree.nwk"
    resources:
        mem_mb = 16000,
        runtime = 12 * 60
    shell:
        """
        cat {params.reference_fasta} {input.new_samples} > input.fasta
        faToVcf -maskSites='{input.masking_vcf}' -ref='{params.reference_id}' input.fasta input.vcf
        usher -T {threads} -i {input.tree_protobuf} -v input.vcf -u -d {params.output_directory}
        cp {params.output_directory}/uncondensed-final-tree.nh {output.tree}
        """


rule build_phylo4:
    threads: 1
    shadow: "shallow"
    conda: "envs/trees.yaml"
    input:
        tree = OUTPUT_DIR/"{dataset}/tree.nwk",
        extracted_ids = OUTPUT_DIR/"{dataset}/extracted_ids.csv"
    output:
        tree_p4 = OUTPUT_DIR/"{dataset}/tree.p4.csv"
    resources:
        mem_mb = 8000,
        runtime = 24 * 60
    script:
        "scripts/build_phylo4.R"


rule calculate_clusters:
    threads: 1
    shadow: "shallow"
    conda: "envs/tcfinder.yaml"
    input:
        tree_p4 = OUTPUT_DIR/"{dataset}/tree.p4.csv",
        targets = OUTPUT_DIR/"{dataset}/targets.txt"
    params:
        min_prop = config["MIN_PROP"],
        min_size = config["MIN_SIZE"]
    output:
        clusters = OUTPUT_DIR/"{dataset}/clusters.csv"
    resources:
        mem_mb = lambda wildcards, attempt: 8000 * attempt,
        runtime = 12 * 60
    retries: 3
    shell: "tcfinder -i {input.tree_p4} -t {input.targets} -o {output.clusters} -p {params.min_prop} -s {params.min_size}"


rule summarize_results:
    threads: 1
    shadow: "shallow"
    conda: "envs/tables.yaml"
    input:
        clusters = OUTPUT_DIR/"{dataset}/clusters.csv",
        extracted_ids = OUTPUT_DIR/"{dataset}/extracted_ids.csv",
        ids_file = INPUT_DIR/"{dataset}.txt"
    output:
        table = OUTPUT_DIR/"{dataset}/summary.json"
    resources:
        mem_mb = 4000,
        runtime = 15
    script:
        "scripts/summarize_results.py"


rule build_haplotype_metadata:
    threads: 1
    conda: "envs/trees.yaml"
    input:
        clusters = OUTPUT_DIR/"{dataset}/clusters.csv",
        extracted_ids = OUTPUT_DIR/"{dataset}/extracted_ids.csv",
        full_metadata = config["METADATA"]
    params:
        metadata_originalid_column = config["METADATA_COLS"]["FULL_METADATA"]["ORIGINAL_ID"],
        metadata_id_column = config["METADATA_COLS"]["FULL_METADATA"]["UNIQUE_ID"],
        metadata_date_column = config["METADATA_COLS"]["FULL_METADATA"]["DATE"],
        metadata_location_column = config["METADATA_COLS"]["FULL_METADATA"]["LOCATION"],
        metadata_gender_column = config["METADATA_COLS"]["FULL_METADATA"]["GENDER"],
        metadata_lineage_column = config["METADATA_COLS"]["FULL_METADATA"]["LINEAGE"],
        metadata_age_column = config["METADATA_COLS"]["FULL_METADATA"]["AGE"]
    output:
        metadata = OUTPUT_DIR/"{dataset}/metadata.csv"
    resources:
        mem_mb = 16000,
        runtime = 2 * 60
    script: "scripts/build_haplotype_metadata.R"


rule estimate_fitness:
    threads: 1
    input:
        full_metadata = config["METADATA"],
        haplotype_metadata = OUTPUT_DIR/"{dataset}/metadata.csv"
    params:
        metadata_id_column = config["METADATA_COLS"]["FULL_METADATA"]["UNIQUE_ID"],
        metadata_date_column = config["METADATA_COLS"]["FULL_METADATA"]["DATE"],
        metadata_location_column = config["METADATA_COLS"]["FULL_METADATA"]["LOCATION"]
    output:
        fitness = OUTPUT_DIR/"{dataset}/fitness/{days_padding}d/[{prop}]_[{size}]/estimated_fitness.csv"
    script: "scripts/estimate_fitness.py"


rule report_estimated_fitness:
    threads: 1
    conda: "envs/report.yaml"
    input:
        fitnesses = expand(OUTPUT_DIR/"{dataset}/fitness/{{days}}d/[{{prop}}]_[{{size}}]/estimated_fitness.csv", dataset=DATASETS)
    params:
        width_per_haplotype_mm = 50, height_mm = 240
    output:
        report = OUTPUT_DIR/"summary/fitness/{days}d/[{prop}]_[{size}]/report.svg",
        report_simple = OUTPUT_DIR/"summary/fitness/{days}d/[{prop}]_[{size}]/report.simple.svg",
        report_data = OUTPUT_DIR/"summary/fitness/{days}d/[{prop}]_[{size}]/report.csv",
        report_adding = OUTPUT_DIR/"summary/fitness/{days}d/[{prop}]_[{size}]/report_adding.svg",
        report_slicing = OUTPUT_DIR/"summary/fitness/{days}d/[{prop}]_[{size}]/report_slicing.svg",
        summary = OUTPUT_DIR/"summary/fitness/{days}d/[{prop}]_[{size}]/summary.csv",
        report_contrast = OUTPUT_DIR/"summary/fitness/{days}d/[{prop}]_[{size}]/report_contrast.csv"
    script: "scripts/report_estimated_fitness.R"


rule report_haplotype_timeline:
    threads: 1
    conda: "envs/report.yaml"
    input:
        haplotype_metadata = OUTPUT_DIR/"{dataset}/metadata.csv"
    params:
        metadata_date_column = config["METADATA_COLS"]["FULL_METADATA"]["DATE"],
        n_top_countries = 5,
        width_mm = 320, height_mm = 200
    output:
        report = OUTPUT_DIR/"{dataset}/timeline/timeline.svg",
        report_data = OUTPUT_DIR/"{dataset}/timeline/timeline.csv"
    script: "scripts/report_timeline.R"


rule report_summarized_timeline:
    threads: 1
    conda: "envs/report.yaml"
    input:
        haplotype_metadata_tables = expand(OUTPUT_DIR/"{dataset}/metadata.csv", dataset=DATASETS)
    params:
        metadata_date_column = config["METADATA_COLS"]["FULL_METADATA"]["DATE"],
        n_top_countries = 11,
        width_mm = 320, height_per_haplotype_mm = 50
    output:
        report = OUTPUT_DIR/"summary/timeline/timeline.svg",
        report_data = OUTPUT_DIR/"summary/timeline/timeline.csv"
    script: "scripts/report_summarized_timeline.R"


rule build_age_corrected_haplotype_metadata:
    threads: 1
    conda: "envs/report.yaml"
    input:
        haplotype_metadata = OUTPUT_DIR/"{dataset}/metadata.csv"
    params:
        metadata_age_column = config["METADATA_COLS"]["FULL_METADATA"]["AGE"],
        cluster_size_threshold = config["MIN_SIZE"],
        min_age = 0,
        max_age = 120  # Maria Branyas was 117 at the time
    output:
        age_corrected_metadata = OUTPUT_DIR/"{dataset}/age/metadata.csv"
    script: "scripts/build_age_corrected_metadata.R"


rule report_age_differences:
    threads: 1
    conda: "envs/report.yaml"
    input:
        age_corrected_tables = expand(OUTPUT_DIR/"{dataset}/age/metadata.csv", dataset=DATASETS)
    params:
        metadata_age_column = config["METADATA_COLS"]["FULL_METADATA"]["AGE"],
        cluster_size_threshold = config["MIN_SIZE"],
        width_per_haplotype_mm = 50, height_mm = 300
    output:
        report_haplotype = OUTPUT_DIR/"summary/age/haplotype_age.svg",
        report_haplotype_contrast = OUTPUT_DIR/"summary/age/haplotype_age.contrast.csv",
        report_haplotype_simple = OUTPUT_DIR/"summary/age/haplotype_age.simple.svg",
        report_transmission = OUTPUT_DIR/"summary/age/transmission_age.svg"
    script: "scripts/report_age_differences.R"


rule report_age_vs_background:
    threads: 1
    conda: "envs/report.yaml"
    input:
        age_corrected_metadata = OUTPUT_DIR/"{dataset}/age/metadata.csv",
        full_metadata = config["METADATA"]
    params:
        metadata_age_column = config["METADATA_COLS"]["FULL_METADATA"]["AGE"],
        metadata_date_column = config["METADATA_COLS"]["FULL_METADATA"]["DATE"],
        metadata_id_column = config["METADATA_COLS"]["FULL_METADATA"]["UNIQUE_ID"],
        min_age = rules.build_age_corrected_haplotype_metadata.params.min_age,
        max_age = rules.build_age_corrected_haplotype_metadata.params.max_age,
        width_mm = 320, height_mm = 200
    output:
        report = OUTPUT_DIR/"{dataset}/age/vs_background.svg",
        report_data = OUTPUT_DIR/"{dataset}/age/vs_background.csv"
    script: "scripts/report_age_vs_background.R"


rule report_missing_data:
    threads: 1
    conda: "envs/report.yaml"
    input:
        age_corrected_tables = expand(OUTPUT_DIR/"{dataset}/age/metadata.csv", dataset=DATASETS)
    params:
        metadata_age_column = config["METADATA_COLS"]["FULL_METADATA"]["AGE"],
        metadata_date_column = config["METADATA_COLS"]["FULL_METADATA"]["DATE"],
        metadata_location_column = config["METADATA_COLS"]["FULL_METADATA"]["LOCATION"],
        width_mm = 320, height_mm = 200
    output:
        report_age = OUTPUT_DIR/"summary/missing/age.svg",
        report_location = OUTPUT_DIR/"summary/missing/location.svg",
        report_date = OUTPUT_DIR/"summary/missing/date.svg"
    script: "scripts/report_missing_data.R"
