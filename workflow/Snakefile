import re
import json
import requests
from pathlib import Path

configfile: "config/config.yaml"


def read_fasta_IDs(path):
    ids = []
    with open(path) as f:
        for line in f:
            if line.startswith(">"):
                ids.append(line[1:].strip().split(" ")[0])
    return ids


def download_file(url, path):
    response = requests.get(url)
    with open(path, "wb") as fw:
        fw.write(response.content)


def copy_file(from_path, to_path, chunk_size=1024):
    with open(from_path, "rb") as f:
        with open(to_path, "wb") as fw:
            fw.write(f.read())


INPUT_DIR = Path(config["INPUT_DIR"])
OUTPUT_DIR = Path(config["OUTPUT_DIR"])
DATASETS = [path.stem for path in INPUT_DIR.glob("*.txt")]


rule all:
    input:
        expand(OUTPUT_DIR/"{dataset}/clusters.csv", dataset=DATASETS),
        expand(OUTPUT_DIR/"{dataset}/summary.json", dataset=DATASETS),
        expand(OUTPUT_DIR/"{dataset}/fitness/{days}d/estimated_fitness.csv", dataset=DATASETS, days=config["FITNESS_PADDING_DAYS"]),
        expand(OUTPUT_DIR/"summary/fitness/{days}d/report.svg", days=config["FITNESS_PADDING_DAYS"]),
        expand(OUTPUT_DIR/"{dataset}/timeline.svg", dataset=DATASETS)


rule get_problematic_vcf:
    threads: 1
    shadow: "shallow"
    output:
        vcf_path = "data/problematic_sites_sarsCov2.vcf"
    resources:
        mem_mb = 2000,
        runtime = 15
    retries: 3
    run:
        if config["PROBLEMATIC_VCF"].startswith("https://"):
            download_file(config["PROBLEMATIC_VCF"], output.vcf_path)
        else:
            copy_file(config["PROBLEMATIC_VCF"], output.vcf_path)


rule get_reference_tree:
    threads: 1
    shadow: "shallow"
    output:
        tree_path = "data/public.all.masked.pb.gz"
    resources:
        mem_mb = 2000,
        runtime = 15
    retries: 3
    run:
        if config["REFERENCE_TREE"].startswith("https://"):
            download_file(config["REFERENCE_TREE"], output.tree_path)
        else:
            copy_file(config["REFERENCE_TREE"], output.tree_path)


rule extract_records:
    threads: 1
    shadow: "shallow"
    conda: "envs/bio.yaml"
    input:
        ids_file = INPUT_DIR/"{dataset}.txt"
    params:
        full_fasta = config["ALIGNED_FULL_FASTA"]
    output:
        fasta = OUTPUT_DIR/"{dataset}/sequences.aligned.fasta",
        extracted_ids = OUTPUT_DIR/"{dataset}/extracted_ids.csv",
        targets = OUTPUT_DIR/"{dataset}/targets.txt"
    resources:
        mem_mb = 4000,
        runtime = 60
    script:
        "scripts/extract_records.py"


rule phylogenetic_placement:
    threads: 8
    shadow: "shallow"
    conda: "envs/usher.yaml"
    input:
        new_samples = OUTPUT_DIR/"{dataset}/sequences.aligned.fasta",
        masking_vcf = "data/problematic_sites_sarsCov2.vcf",
        tree_protobuf = "data/public.all.masked.pb.gz"
    params:
        reference_fasta = config["REFERENCE_FASTA"],
        reference_id = read_fasta_IDs(config["REFERENCE_FASTA"])[0],
        output_directory = "pp_out"  # if shadow == "shallow", it will be removed
    output:
        tree = OUTPUT_DIR/"{dataset}/tree.nwk"
    resources:
        mem_mb = 16000,
        runtime = 12 * 60
    shell:
        """
        cat {params.reference_fasta} {input.new_samples} > input.fasta
        faToVcf -maskSites='{input.masking_vcf}' -ref='{params.reference_id}' input.fasta input.vcf
        usher -T {threads} -i {input.tree_protobuf} -v input.vcf -u -d {params.output_directory}
        cp {params.output_directory}/uncondensed-final-tree.nh {output.tree}
        """


rule build_phylo4:
    threads: 1
    shadow: "shallow"
    conda: "envs/trees.yaml"
    input:
        tree = OUTPUT_DIR/"{dataset}/tree.nwk",
        extracted_ids = OUTPUT_DIR/"{dataset}/extracted_ids.csv"
    output:
        tree_p4 = OUTPUT_DIR/"{dataset}/tree.p4.csv"
    resources:
        mem_mb = 8000,
        runtime = 24 * 60
    script:
        "scripts/build_phylo4.R"


rule calculate_clusters:
    threads: 1
    shadow: "shallow"
    conda: "envs/tcfinder.yaml"
    input:
        tree_p4 = OUTPUT_DIR/"{dataset}/tree.p4.csv",
        targets = OUTPUT_DIR/"{dataset}/targets.txt"
    params:
        min_prop = config["MIN_PROP"],
        min_size = config["MIN_SIZE"]
    output:
        clusters = OUTPUT_DIR/"{dataset}/clusters.csv"
    resources:
        mem_mb = lambda wildcards, attempt: 8000 * attempt,
        runtime = 12 * 60
    retries: 3
    shell: "tcfinder -i {input.tree_p4} -t {input.targets} -o {output.clusters} -p {params.min_prop} -s {params.min_size}"


rule summarize_results:
    threads: 1
    shadow: "shallow"
    conda: "envs/tables.yaml"
    input:
        clusters = OUTPUT_DIR/"{dataset}/clusters.csv",
        extracted_ids = OUTPUT_DIR/"{dataset}/extracted_ids.csv",
        ids_file = INPUT_DIR/"{dataset}.txt"
    output:
        table = OUTPUT_DIR/"{dataset}/summary.json"
    resources:
        mem_mb = 4000,
        runtime = 15
    script:
        "scripts/summarize_results.py"


rule build_haplotype_metadata:
    threads: 1
    conda: "envs/trees.yaml"
    input:
        clusters = OUTPUT_DIR/"{dataset}/clusters.csv",
        extracted_ids = OUTPUT_DIR/"{dataset}/extracted_ids.csv",
        full_metadata = config["METADATA"]
    params:
        metadata_originalid_column = config["METADATA_COLS"]["FULL_METADATA"]["ORIGINAL_ID"],
        metadata_id_column = config["METADATA_COLS"]["FULL_METADATA"]["UNIQUE_ID"],
        metadata_date_column = config["METADATA_COLS"]["FULL_METADATA"]["DATE"],
        metadata_location_column = config["METADATA_COLS"]["FULL_METADATA"]["LOCATION"],
        metadata_gender_column = config["METADATA_COLS"]["FULL_METADATA"]["GENDER"],
        metadata_lineage_column = config["METADATA_COLS"]["FULL_METADATA"]["LINEAGE"],
        metadata_age_column = config["METADATA_COLS"]["FULL_METADATA"]["AGE"]
    output:
        metadata = OUTPUT_DIR/"{dataset}/metadata.csv"
    resources:
        mem_mb = 16000,
        runtime = 2 * 60
    script: "scripts/build_haplotype_metadata.R"


rule estimate_fitness:
    threads: 1
    input:
        full_metadata = config["METADATA"],
        haplotype_metadata = OUTPUT_DIR/"{dataset}/metadata.csv"
    params:
        metadata_id_column = config["METADATA_COLS"]["FULL_METADATA"]["UNIQUE_ID"],
        metadata_date_column = config["METADATA_COLS"]["FULL_METADATA"]["DATE"],
        metadata_location_column = config["METADATA_COLS"]["FULL_METADATA"]["LOCATION"]
    output:
        fitness = OUTPUT_DIR/"{dataset}/fitness/{days_padding}d/estimated_fitness.csv"
    script: "scripts/estimate_fitness.py"


rule report_summarized_estimated_fitness:
    threads: 1
    conda: "envs/report.yaml"
    input:
        fitnesses = expand(OUTPUT_DIR/"{dataset}/fitness/{{days}}d/estimated_fitness.csv", dataset=DATASETS)
    params:
        width_mm = 320, height_mm = 240
    output:
        report = OUTPUT_DIR/"summary/fitness/{days}d/report.svg",
        report_data = OUTPUT_DIR/"summary/fitness/{days}d/report.csv",
        summary = OUTPUT_DIR/"summary/fitness/{days}d/summary.csv",
    script: "scripts/report_estimated_fitness.R"


rule report_haplotype_timeline:
    threads: 1
    conda: "envs/report.yaml"
    input:
        haplotype_metadata = OUTPUT_DIR/"{dataset}/metadata.csv"
    params:
        metadata_lineage_column = config["METADATA_COLS"]["FULL_METADATA"]["LINEAGE"],
        metadata_date_column = config["METADATA_COLS"]["FULL_METADATA"]["DATE"],
        width_mm = 320, height_mm = 100
    output:
        report = OUTPUT_DIR/"{dataset}/timeline.svg",
        report_data = OUTPUT_DIR/"{dataset}/timeline.csv"
    script: "scripts/report_timeline.R"
